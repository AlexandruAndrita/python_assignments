{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(0,120,170)\">Assignment 4: Decision Trees</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This material, no matter whether in printed or electronic form,\n",
    "may be used for personal and non-commercial educational use\n",
    "only. Any reproduction of this material, no matter whether as a\n",
    "whole or in parts, no matter whether in printed or in electronic\n",
    "form, requires explicit prior acceptance of the authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Automatic Testing Guidelines</h2>\n",
    "\n",
    "Automatic unittesting requires you, as a student, to submit a notebook which contains strictly defined objects.\n",
    "Strictness of definition consists of unified shapes, dtypes, variable names, and more.\n",
    "\n",
    "Within the notebook, we provide detailed instructions which you should follow in order to maximize your final grade. Please keep in mind:\n",
    "\n",
    "* Don't add any cells but use the ones provided by us. You may notice that most cells are tagged such that the unittest routine can recognise them.\n",
    "\n",
    "* We highly recommend you to develop your code within the provided cells. You can implement helper functions where needed as long as you put them in the same cell they are actually called. Always make sure that implemented functions have the correct output and given variables contain the correct data type. Don't import any other packages than listed in the cell with the \"imports\" tag.\n",
    "\n",
    "* Never use variables you defined in another cell in your functions directly; always pass them to the function as a parameter. In the unittest they won't be available either.\n",
    "\n",
    "*Good luck! :)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a starting point: For the notebook to compile without problems, make sure that the required data set provided with the link to Google drive is stored in a folder \"dataset\"; also make sure that this folder and the additional Python file provided via Moodle are in the same folder as the notebook! Unfortunately the data set is too big to upload it to Moodle directly...\n",
    "\n",
    "**Note:** The execution of this notebook will take a while, i.e., it might run for 20-30 minutes. For testing purposes, you might work with a reduced dataset (Task 4) but make sure to report the numbers etc. for the whole dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Task 1: Gini Impurity (20 points)</h2>\n",
    "\n",
    "In this task, we will recall the most important concepts of decision trees by walking you through a simple example. On the way you have to solve some exercises to gain basic insights. Let's start with a toy dataset for one tree.\n",
    "<br> <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "imports"
    ]
   },
   "outputs": [],
   "source": [
    "# nothing to do here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import model_selection, svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from mnist_loader import MNIST\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Set random seed to ensure reproducible runs\n",
    "RSEED = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple data set for demonstration \n",
    "# nothing to do here\n",
    "X = np.array([[2, 2], \n",
    "              [2, 1],\n",
    "              [2, 3], \n",
    "              [1, 2], \n",
    "              [1, 1],\n",
    "              [3, 3],\n",
    "              [3, 2]])\n",
    "\n",
    "y = np.array([0, 1, 1, 1, 0, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nothing to do here. Just execute the command to gain a proper visualization!\n",
    "%matplotlib inline\n",
    "\n",
    "# plot each point as the label\n",
    "for x1, x2, label in zip(X[:, 0], X[:, 1], y):\n",
    "    plt.text(x1, x2, str(label), fontsize = 40, color = 'r',\n",
    "             ha='center', va='center')\n",
    "    \n",
    "# plot formatting\n",
    "plt.grid(None)\n",
    "plt.xlim((0, 3.5))\n",
    "plt.ylim((0, 3.5))\n",
    "plt.xlabel('x1', size = 20); plt.ylabel('x2', size = 20); plt.title('Demonstration Data', size = 24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **Decision Tree Classifier (DTC)** builds a decision tree based on the features of the data. This is equivalent to subdividing the feature space. Let's consider the example above and apply a simple heuristics. In the first step we try to subdivide the space such that we obtain the _largest possible leaf (subdivision)_ that contains only **one class**.\n",
    "<br><br>\n",
    "We first look at the feature $x_2$, i.e. a horizontal division of the space. For example, we could divide the space at the specific threshold $x_2 = 2.5$. Then we end up having a group of samples with features $x_2 > 2.5$ and homogenous class label 1, i.e. the two points with coordinates $\\{ (2,3), (3,3) \\}$ . If we instead look at the feature $x_1$, corresponding to a vertical division of the space, we cannot find an equally large or larger group of samples with the same label. Therefore, our first node in the tree is: $x_2 \\leq 2.5$, i.e. we split the space and repeat the same procedure on each of the two leafs. In our case we are done with the top leaf (since both samples have the same class) and only need to repeat the procedure on the bottom leaf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our simple heuristics from above fails on the bottom node. We need a better criterion to decide which splits to make. Nowadays the most frequently used one is called the Gini Impurity. \n",
    "\n",
    "The **Gini Impurity** is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset.\n",
    "<br>\n",
    "What does that mean? Let us suppose we have $2$ labels and let $p_1, p_2$ be the fractions of points labeled with labels $1$ and $2$ (note: $p_1 + p_2 = 1$) . The probabilty to choose a point with label $1$ is $p_1$. \n",
    "The probability to choose label $2$ is $p_2 = 1-p_1$. Therefore, the probability to label a point of label $1$ with label $2$ is $p_1 \\cdot p_2 = p_1 \\cdot (1-p_1) = p_1 - p_1^2$. \n",
    "<br>\n",
    "Analogously, the probability for points with label $2$ to be labeled with $1$ is $p_2 \\cdot (1-p_2) = p_2 - p_2^2$. The Gini Impurity is the sum over both: $p_1 - p_1^2 + p_2 - p_2^2 = p_1 + p_2 - p_1^2 - p_2^2 = 1 - p_1^2 -p_2^2$\n",
    "<br>\n",
    "The above reasoning is easy to generalize to the case where the number of labels $M$ is larger then two: $M > 2$ . The formula for the given dataset $Z$ is simply\n",
    "$I_G(Z) = 1 - \\sum_{k = 1}^M p_k(Z)^2,$ where $p_k(Z)$ is the frequency of points with labels $k$ in the dataset $Z$. Your task will now be to apply these techniques explicitly to the toy data set introduced above.\n",
    "<br>\n",
    "\n",
    "The **Gini Impurity Gain** is the amount of \"impurity\" we get rid of for a specific split $s$.\n",
    "Let's assume that we get the partition $Z_{s,1}, \\ldots, Z_{s,K_s}$ of $Z$ after applying $s$. Then the impurity gain is\n",
    "$g_G(Z,s) = I_G(Z) - \\sum_{t=1}^{K_s} \\frac{|Z_{s,t}|}{|Z|} \\cdot I_G(Z_{s,t})$\n",
    "\n",
    "**Task 1.1:**\n",
    "\n",
    "*  Calculate the Gini impurity for our toy dataset\n",
    "* Calculate the Gini impurity for the top and bottom leaf in our split (split $x_2$ at $2.5$)\n",
    "* Calculate the Gini impurity gain for our split in the toy dataset \n",
    "\n",
    "To do this, implement the necessary calculations into the function `calc_gini` and return the 4 solutions.<br>\n",
    "**Note:** Your implementation should work for any dataset similar to the toy dataset (i.e. binary labels, two dimensional)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(210,90,80)\">1.1 Calculation (20 points):</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "gini_calc"
    ]
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function that calculates the Gini Impurity of the whole dataset and of the two datasets after a split is performed.\n",
    "Returns also the impurity gain from this specific split.\n",
    "@param X, np ndarray, data matrix\n",
    "@param y, np ndarray, data vector\n",
    "@param split, float, value for splitting\n",
    "@param entry, int, indicating at which entry/axis/dimension the split should be performed (e.g. if entry=0 then you\n",
    "would perform the split for the first feature, for entry=1 it's the second feature)\n",
    "\"\"\"\n",
    "def calc_gini(X,y,split,entry):    \n",
    "    \n",
    "    # replace the following line with your lines of code\n",
    "    raise NotImplementedError(\"You have not implemented this function\")\n",
    "    \n",
    "    return gini_impurity, gini_top, gini_bottom, impurity_gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gini_impurity, gini_top, gini_bottom, impurity_gain = calc_gini(X,y,2.5,1)\n",
    "print(\"Gini impurity: {}\\nGini impurity top leaf: {}\\nGini impurity bottom leaf: {}\\nGini impurity gain: {}\".format(gini_impurity, gini_top, gini_bottom, impurity_gain))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Task 2: Train a simple decision tree (10 points)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you should provide a Python routine for the previous example. In the cells below there is the function `dec_tree` where you have to implement the following for **Task 2.1**:\n",
    "* Train a decision tree on the dataset $X$ from Task 1 and remember to pass the random seed `RSEED` defined in the beginning.\n",
    "* Return the number of tree nodes, the maximum depth of the tree, and the accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(210,90,80)\">2.1 Code (10 points):</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "dec_tree"
    ]
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Trains a decision tree and returns certain attributes of the received model.\n",
    "@param seed, int, seed for preserving reproducibility of random events\n",
    "@param X, np ndarray, data matrix\n",
    "@param y, np ndarray, data vector\n",
    "\"\"\"\n",
    "# Make a decision tree and train\n",
    "def dec_tree(seed,X,y):\n",
    "    \n",
    "    # replace the following line with your lines of code\n",
    "    raise NotImplementedError(\"You have not implemented this function\")\n",
    "    \n",
    "    return nr_nodes, max_depth, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print number of tree nodes and the maximum depth of tree\n",
    "nr_nodes, max_depth, acc = dec_tree(RSEED,X,y)\n",
    "print(f'Decision tree has {nr_nodes} nodes with maximum depth {max_depth}.')\n",
    "print(f'Model accuracy: {acc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Task 3: Decision tree on a real data set (25 points)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will apply the different classifiers we have encountered so far to a real-world benchmark data set which is often used in applications, namely the \"Fashion MNIST\" dataset. It consists of images of clothing, like sneakers and shirts. It was created to be an alternative to the famous MNIST benchmark dataset, which is nowadays considered as too easy for the most recent algorithms. Let us first load the train and test set using the files provided in Moodle. The train and test data are represented as pixel arrays, and the label vector indicates the different classes (0 to 9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nothing to do here. Just execute the command!\n",
    "data = MNIST('./dataset/')\n",
    "img_train, labels_train = data.load_training()\n",
    "x_train = np.array(img_train)\n",
    "y_train = np.array(labels_train)\n",
    "\n",
    "img_test, labels_test = data.load_testing()\n",
    "x_test = np.array(img_test)\n",
    "y_test = np.array(labels_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot some of the images so that you see, how these data look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# nothing to do here. Just execute the command!\n",
    "arr = ['T-Shirt/Top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle Boot']\n",
    "plt.figure(figsize=(25, 9))\n",
    "for i in range(30):\n",
    "    plt.subplot(3, 10, i + 1)\n",
    "    two_d = (np.reshape(x_train[i], (28, 28))).astype(np.uint8)\n",
    "    plt.imshow(two_d, interpolation='nearest',cmap='gray')\n",
    "    plt.title('Label: {0}'.format(arr[y_train[i]]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we provide you with a routine that trains a decision tree for the given training set. The function `get_evaluation` should additionally help you to compute accuracies and provide confusion matrices and appropriate heatmaps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "prep_clf"
    ]
   },
   "outputs": [],
   "source": [
    "# nothing to do here. Just execute the command!\n",
    "print('\\nPreparing classifier...')\n",
    "model = DecisionTreeClassifier(criterion=\"gini\", max_depth=50, splitter=\"best\", random_state=RSEED)\n",
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "get_evaluation"
    ]
   },
   "outputs": [],
   "source": [
    "# nothing to do here. Just execute the command!\n",
    "\"\"\"\n",
    "Evaluates the model and returns accuracy as well as a confusion matrix. Also the time for prediction can is calculated.\n",
    "@param model, sklearn model, trained model\n",
    "@param x_test, np ndarray, data matrix\n",
    "@param y_test, np ndarray, data vector\n",
    "\"\"\"\n",
    "def get_evaluation(model, x_test, y_test):\n",
    "    start = time.time()\n",
    "    y_pred = model.predict(x_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    conf_mat = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    print('\\nPredicted values: ', y_pred)\n",
    "    print('\\nAccuracy of classifier on test image data: ', accuracy)\n",
    "    print('\\nConfusion matrix: \\n', conf_mat)\n",
    "    print('\\nTime: ', time.time()-start)\n",
    "\n",
    "    plt.matshow(conf_mat)\n",
    "    plt.title('Confusion matrix')\n",
    "    plt.colorbar()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nothing to do here. Just execute the command!\n",
    "_ = get_evaluation(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to obtain a better performance, we apply a hyperparameter search in **Task 3.1**. \n",
    "* To this end create a parameter grid (dictionary) which iterates over the following quantities:\n",
    "    - `criterion`: 'gini' and 'entropy'\n",
    "    - `max_depth`: 10, 50 and 100\n",
    "    - `splitter`: 'random' and 'best'\n",
    "    - **Hint:** Have a look at the documentation of sklearn.model_selection.RandomizedSearchCV to get an idea of how this parameter grid should look like.\n",
    "* Use a decision tree classifier and RandomizedSearchCV with 5 iterations and 3 fold cross validation. Use the built-in routines from sklearn for this and don't forget to pass `random_state=RSEED`.\n",
    "* Evaluate the best parameter combination from this model.\n",
    "* Print the accuracy and plot the confusion matrices and heatmaps of the model evaluated at the test set (use the previously implemented routine `get_evaluation` for this).\n",
    "\n",
    "Again, don't forget to pass the seed in the **decision tree classifier** and **RandomizedSearchCV**. Warning: this may take several minutes ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(210,90,80)\">3.1 Code (25 points):</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "train_dec_tree"
    ]
   },
   "outputs": [],
   "source": [
    "# your solution\n",
    "param_dict_grid = None\n",
    "\n",
    "\"\"\"\n",
    "Trains a decision tree using cross-validation and returns certain attributes of the received model including the best\n",
    "parameter combination.\n",
    "@param x_train, np ndarray, data matrix\n",
    "@param y_train, np ndarray, data vector\n",
    "@param param_grid, dict, grid holding the paramaters for search\n",
    "@param seed, int , seed for preserving reducibility of random events\n",
    "@param n_iter, int, number of iterations (RandomizedSearchCV)\n",
    "@param cv, in, number of folds (RandomizedSearchCV)\n",
    "\"\"\"\n",
    "def train_dec_tree(x_train,y_train,param_dict_grid,seed,n_iter,cv):\n",
    "    \n",
    "    # replace the following line with your lines of code\n",
    "    raise NotImplementedError(\"You have not implemented this function\")\n",
    "    \n",
    "    return model.best_params_,model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_dec_tree, model_dec_tree = train_dec_tree(x_train,y_train,param_dict_grid,RSEED,5,3)\n",
    "_ = get_evaluation(model_dec_tree, x_test, y_test)\n",
    "print(\"The best parameters are: {}\".format(params_dec_tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did the task correctly, you should obtain a slightly better result than before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Task 4: Comparison with KNN and SVMs (30 points)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task we make a comparison to the other previously used classifiers, i.e. KNNs and SVMs. Similarly as for the tree, on the Fashion MNIST dataset:\n",
    "\n",
    "* **Task 4.1**: Implement a KNN classifier with `n_neighbors=5`, `weights='distance'`, and `p=1`. Print the accuracy and plot the confusion matrices and heatmaps of the model evaluated at the test set (again, you can use the previously implemented function `get_evaluation` for this). \n",
    "* **Task 4.2**: Implement a SVM classifier with `C=10`, `kernel='poly'`, and `gamma='auto'`. Print the accuracy and plot the confusion matrices and heatmaps of the model evaluated at the test set (again, you can use the previously implemented function `get_evaluation` for this). \n",
    "\n",
    "This may also take some time!\n",
    "\n",
    "Afterwards, answer some questions that correspond to your observations (**Task 4.3**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(210,90,80)\">4.1 Code (10 points):</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "KNN"
    ]
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Trains a KNN classifier on the given dataset.\n",
    "@param x_train, np ndarray, data matrix\n",
    "@param y_train, np ndarray, data vector\n",
    "@param n_neighbors, int, number of neighbors\n",
    "@param weights, str, mode for weights\n",
    "@param p, float, power parameter for the Minkowski metric (see documentation: neighbors.KNeighborsClassifier)\n",
    "\"\"\"\n",
    "def trainKNN(x_train,y_train,n_neighbors,weights,p):\n",
    "    \n",
    "    # replace the following line with your lines of code\n",
    "    raise NotImplementedError(\"You have not implemented this function\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model = trainKNN(x_train,y_train,5,'distance',1)\n",
    "_ = get_evaluation(knn_model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(210,90,80)\">4.2 Code (10 points):</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "SVM"
    ]
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Trains an SVM classifier on the given dataset.\n",
    "@param x_train, np ndarray, data matrix\n",
    "@param y_train, np ndarray, data vector\n",
    "@param kernel, str, type of kernel being used\n",
    "@param gamma, float, kernel coefficient\n",
    "\"\"\"\n",
    "def trainSVM(x_train,y_train,C,kernel,gamma,seed):\n",
    "    \n",
    "    # replace the following line with your lines of code\n",
    "    raise NotImplementedError(\"You have not implemented this function\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = trainSVM(x_train,y_train,10,'poly','auto',RSEED)\n",
    "_ = get_evaluation(svm_model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(210,90,80)\">4.3 Code (5 points):</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats, you made it this far! Now let's put everything together :)\n",
    "\n",
    "- Collect all the accuracies from the different classifiers as well as the best parameters for the decision tree (from Task 3).\n",
    "- Then, in the next cell, plot the accuracy of each classifier to compare them. Make sure to only access data that you put into the cell below (with the tag \"value_check\").\n",
    "\n",
    "**Note:** Please assign the numerical values to the variables in float format (i.e. 0.9876 for 98.76% accuracy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "value_check"
    ]
   },
   "outputs": [],
   "source": [
    "# for your reference:\n",
    "# Accuracy:\n",
    "classifier1 = 0.9876\n",
    "# Parameters:\n",
    "parameter1 = 'split'\n",
    "\n",
    "dec_tree_fixed_params_acc = 0.7983\n",
    "\n",
    "# your solution:\n",
    "dec_tree_best_params_acc = 0\n",
    "KNN_acc = 0\n",
    "SVM_acc = 0\n",
    "\n",
    "# Best parameters dec_tree:\n",
    "par_splitter_ = None\n",
    "par_maxdepth_ = None\n",
    "par_criterion_ = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "plot"
    ]
   },
   "outputs": [],
   "source": [
    "# your code for the visualization\n",
    "\n",
    "# example: plt.figure...\n",
    "# plt.plot..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(210,90,80)\">4.4 Question (5 points):</h3>\n",
    "\n",
    "***What observations can you make so far? Add your answer to the variables below (several may be correct).***\n",
    "\n",
    "According to the `get_evaluation` function:\n",
    "\n",
    "a_) The decision tree model is faster in terms of inference time and yields a better accuracy than KNN and SVM. <br>\n",
    "b_) SVM is the algorithm with the highest accuracy among KNN, SVM, and decision trees.<br>\n",
    "c_) Although this is already a large dataset, it is surprising that the SVM with kernel has higher prediction time than the decision tree.\n",
    "\n",
    "To answer the question, assign \"True\" or \"False\" boolean values to variables in the next cell. A non-correctly answered question yields negative points and no answer (i.e. answer “None”) gives 0 points for a question. More details on grading can be found in the [FAQ sheet](https://docs.google.com/document/d/11ccAoEWh1APAoj79kGFiL64_7OL7RApPUHJ2-cvS2s0/edit?usp=sharing).<br>\n",
    "<b>Note:</b> Do not reuse these variable names. They are used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "q1"
    ]
   },
   "outputs": [],
   "source": [
    "#examples for you\n",
    "example_of_true_variable = True\n",
    "example_of_false_variable = False\n",
    "\n",
    "#your answers go here\n",
    "a_=None\n",
    "b_=None\n",
    "c_=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Task 5: Preparation towards ensembles of trees (15 points)</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the upcoming lectures, you will discuss ensemble methods for trees that aggregate and/or average single tree models to achieve better performances and/or faster runtimes compared to the ones we used here. Random Forest is a famous example where we average over trees such that the overall variance (of the average) is reduced. We will now formalize the situation:\n",
    "\n",
    "Let's say you have $X_1,...,X_B$ identically distributed random variables which are NOT necessarily independent. Let us denote the variance of a single variable $X_i$ by $\\sigma^2$ and the correlation coefficient between two $X_i$ and $X_j$ for $j \\ne i$ by $\\rho=\\frac{E(X_i X_j)-E(X_i)E(X_j)}{\\sigma^2}$ (keep in mind that all $X_i$'s are identically distributed!). \n",
    "\n",
    "In **Task 5.1**, show that $$\\text{Var}\\left(\\frac{1}{B} \\sum_{i=1}^B X_i \\right)=\\rho \\sigma^2 +\\frac{1-\\rho}{B} \\sigma^2. \\quad (1)$$\n",
    "This gives some intuition about how to control the overall variance of averages. One can reduce it e.g. by a small correlation coefficient $\\rho$ and a large number of models $B$.\n",
    "\n",
    "1. Apply the definition of the variance to the given average. \n",
    "2. Split up the resulting double sum in parts with equal and unequal indices. \n",
    "3. Apply the definition for $\\rho$ and replace the sums with the number of occurences of the respective term (how often the indices appear).\n",
    "4. Rewrite this to get the desired solution.\n",
    "\n",
    "**Note:** Denote in your calculation where you tackle each of this points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(210,90,80)\">5.1 Calculation (15 points):</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "calculation_latex"
    ]
   },
   "source": [
    "Your calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "exec"
    ]
   },
   "outputs": [],
   "source": [
    "# executability check\n",
    "calc_gini(np.eye(2),np.ones(2),0.5,1)\n",
    "dec_tree(RSEED,np.ones((2,2)),np.ones(2))\n",
    "train_dec_tree(np.ones((10,2)),np.ones(10),{\"max_depth\":[1,2,4,5,6]},RSEED,10,10)\n",
    "trainKNN(np.ones((10,2)),np.ones(10),1,'distance',1)\n",
    "trainSVM(np.ones((4,2)),np.array([0,1,0,1]),1,'poly','auto',RSEED)\n",
    "print(\"Executable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The execution of this notebook will take a while, i.e., it might run for 20-30 minutes. For testing purposes, you might work with a reduced dataset (Task 4) but make sure to report the numbers etc. for the whole dataset!"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "356734b0b4e05b3af569ed06eb258f6ef66038e7268c6bdbb97ecd1a1c609e88"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
