{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color:rgb(0,120,170)\">Assignment 3: Constrained Optimization, SVMs</h1><br>\n",
    "<h2 style=\"color:rgb(0,120,170)\">Copyrighting and Fare Use</h2>\n",
    "\n",
    "This material, no matter whether in printed or electronic form,\n",
    "may be used for personal and non-commercial educational use\n",
    "only. Any reproduction of this material, no matter whether as a\n",
    "whole or in parts, no matter whether in printed or in electronic\n",
    "form, requires explicit prior acceptance of the authors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:rgb(0,120,170)\">Automatic Testing Guidelines</h2>\n",
    "\n",
    "Automatic unittesting requires you, as a student, to submit a notebook which contains strictly defined objects.\n",
    "Strictness of definition consists of unified shapes, dtypes, variable names and more.\n",
    "\n",
    "Within the notebook, we provide detailed instruction which you should follow in order to maximise your final grade.\n",
    "\n",
    "**Name your notebook properly**, follow the pattern in template name:\n",
    "\n",
    "**Assignment_N_NameSurname_matrnumber**\n",
    "<ol>\n",
    "    <li>N - number of assignment</li>\n",
    "    <li>NameSurname - your full name where every part of the name starts with a capital letter, no spaces</li>\n",
    "    <li>matrnumber - your 8-digit student number on ID card (without k)</li>\n",
    "</ol>\n",
    "\n",
    "**Example:**<br>\n",
    " ✅ Assignment_0_RenéDescartes_12345678<br>\n",
    " ✅ Assignment_0_SørenAabyeKierkegaard_12345678<br>\n",
    " ❌ Assignment0_Peter_Pan_k12345678\n",
    "\n",
    "Don't add any cells but use the ones provided by us. You may notice that most cells are tagged such that the unittest routine can recognise them.\n",
    "\n",
    "We highly recommend you to develop your code within the provided cells. You can implement helper functions where needed unless you put them in the same cell they are actually called. Always make sure that implemented functions have the correct output and given variables contain the correct data type. Don't import any other packages than those listed in the cell with the \"imports\" tag.\n",
    "\n",
    "**Note:** Never use variables you defined in another cell in your functions directly; always pass them to the function as a parameter. In the unitest they won't be available either.\n",
    "\n",
    "*Good luck!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(210,90,80)\">Calculation 1 (25 points):</h3>\n",
    "\n",
    "Consider the following primal problem: \n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Minimize} \\qquad &4(w_1^4+w_2^4)\n",
    "\\\\\n",
    "\\text{subject to} \\qquad &4+w_1-w_2 \\le 0 \\\\\n",
    "\\end{align*}\n",
    "\n",
    "We try to solve it via two ways, thus do the following tasks:\n",
    "* Compute the Lagrangian $L(w_1,w_2,\\alpha)$.\n",
    "* Calculate its derivatives with respect to $w_1$ and $w_2$ and calculate the zeros of these derivatives ($w_1^*,w_2^*$).\n",
    "\n",
    "First way:\n",
    "\n",
    "* Solve the problem using the KKT conditions.\n",
    "\n",
    "Second way:\n",
    "* Write down the dual problem and solve it directly WITHOUT the help of the KKT-conditions.\n",
    "\n",
    "For your calculation use the proposed notation.\n",
    "\n",
    "<h3 style=\"color:rgb(210,90,80)\">Calculation 1 (25 points):</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "calc1"
    ]
   },
   "source": [
    "<ol>\n",
    "    <li>Lagrangian function</li>\n",
    "    $SolutionHere$\n",
    "    <br><br>\n",
    "    <li>Derivatives of Lagrangian</li>\n",
    "    $SolutionHere$\n",
    "    <br><br>\n",
    "    <li>Solving by using KKT conditions</li>\n",
    "    $SolutionHere$\n",
    "    <br><br>\n",
    "    <li>Solving without KKT conditions</li>\n",
    "    $SolutionHere$\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(210,90,80)\">Calculation 2 (25 points):</h3>\n",
    "\n",
    "Suppose we replace in the primal optimization problem of C-SVMs the penalty\n",
    "term $\\|\\xi\\|_1=\\sum_{i=1}^l \\xi_i$ with $\\|\\xi\\|_2^2=\\sum_{i=1}^l \\xi_i^2$ (that is we use the quadratic hinge loss instead). Thus the primal problem we are considering is given as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "\\text{Minimize} \\qquad &\\frac{1}{2}\\|\\mathbf{w}\\|^2+C \\sum_{i=1}^l \\xi_i^2 \\\\\n",
    "\\text { subject to } \\qquad  &y_i\\left(\\mathbf{w} \\cdot \\mathbf{x}_i+b\\right) \\geq 1-\\xi_i \\\\\n",
    "\\text{and} \\qquad &\\xi_i \\geq 0\n",
    "\\end{align*}\n",
    "\n",
    "for all $i=1,...,l$. Give the associated dual optimization problem by making use of the KKT-Theorem and perform the following tasks:\n",
    "\n",
    "* Why can the KKT-Theorem be applied?\n",
    "* Calculate the Lagrange function.\n",
    "* Calculate its derivatives (with respect to $w_i$, $b$ and $\\xi_i$) and compute their zeros. \n",
    "* Write down the dual function (named $\\cal L$ in the slides) and the corresponding dual problem.\n",
    "* Simplify the dual problem so that it only depends on $l$ Lagrange variables. Argue why this can be done.\n",
    "\n",
    "**Please provide reasoning and explanations in full sentences. Grading of the task will heavily depend on it.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "calc2"
    ]
   },
   "source": [
    "<ol>\n",
    "    <li>Why can the KKT-Theorem be applied?</li>\n",
    "    $SolutionHere$\n",
    "    <br><br>\n",
    "    <li>Lagrangian function</li>\n",
    "    $SolutionHere$\n",
    "    <br><br>\n",
    "    <li>Derivatives of Lagrangian</li>\n",
    "    $SolutionHere$\n",
    "    <br><br>\n",
    "    <li>The dual function and the dual problem.</li>\n",
    "    $SolutionHere$\n",
    "    <br><br>\n",
    "    <li>Simplify</li>\n",
    "    $SolutionHere$\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(210,90,80)\">Code 1 (10 points):</h3>\n",
    "\n",
    "The aim of the following task is to equip you with some intuition concerning the application of different SVMs to an easy data set. <br>\n",
    "You should also observe how different versions of the SVMs with different hyperparameters react to additional noise. To this end we provided you a function `plot_data`, that is intended to create proper visualizations of important characteristics of linear and nonlinear SVMs, like the decision border and support vectors. <br>\n",
    "The usual routine for applying SVMs in Python, which is also used here, is given by the following sklearn-package: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html. \n",
    "\n",
    "Your first task is to get familiar with the `plot_data` function by applying it to the easy data set `radial_data.csv`. Iterate over different kernels and hyperparameters (consider the documentation for details):\n",
    "* As a first task plot the dataset using the provided functions. \n",
    "* Using the sklearn-package mentioned above, write a function `iter_Degree` that applies an SVM with a polynomial kernel and $C=10$ to the data and iterates over the degrees from 1 to 5. Function `iter_Degree` must return a list of models' parameters for each model you initiated and fitted on our data. Model parameters is a dictionary, consult the  <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html?highlight=svc#sklearn.svm.SVC.get_params\">sklearn docs</a> . <br>Apply `plot_data` to each hyperparameter setting.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "imports"
    ]
   },
   "outputs": [],
   "source": [
    "#Nothing to do here\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import csv\n",
    "\n",
    "np.random.seed(1234)  \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "mainfuncs"
    ]
   },
   "outputs": [],
   "source": [
    "#Nothing to do here\n",
    "\"\"\"Function allows ro load datapoint from csv\n",
    "@returns: tuple (X,y)\"\"\"\n",
    "def load_data(id_data=1):\n",
    "    if id_data ==1:\n",
    "        Z = np.genfromtxt('radial_data.csv', delimiter=',')\n",
    "        return Z[:,:-1], Z[:,-1]\n",
    "\n",
    "\"\"\"Function creates space/grid. Mostly used for plotting\"\"\"\n",
    "def get_meshgrid(X,resolution):\n",
    "    s = np.max(np.abs(X))*1.05\n",
    "    ls = np.linspace(-s, s, resolution)\n",
    "    X1,X2 = np.meshgrid(ls, ls, sparse=False)\n",
    "    return np.c_[X1.ravel(), X2.ravel()]\n",
    "\n",
    "\"\"\"Plotting your data\n",
    "@param model already trained SVM model, is None if you want to plot data only\n",
    "all other parameters must be intuitively clear for you\"\"\"\n",
    "def plot_data(X, y, \n",
    "              model=None,\n",
    "              plot_boarders=True, \n",
    "              plot_classification=True,  \n",
    "              plot_support_vectors = True,\n",
    "              plot_size=7, \n",
    "              resolution=500, \n",
    "              title='data visualization', \n",
    "              color = ['blue','orange']):\n",
    "    \n",
    "    if model is not None:#if you want to plot model\n",
    "        if plot_classification and plot_boarders:\n",
    "            col=2  #if you want to plot model and boarders\n",
    "        else :\n",
    "            col=1  #if you want to plot model only\n",
    "        \n",
    "        fig,axs = plt.subplots(1,col,figsize=(plot_size*col,plot_size))\n",
    "        \n",
    "        grid = get_meshgrid(X,resolution)        \n",
    "        V = model.support_vectors_\n",
    "        mask_sv = model.support_ #np.where(np.isin(X[:,0],V[:,0]))[0] \n",
    "        \n",
    "        kernel = model.kernel\n",
    "        if kernel == 'poly':\n",
    "            title = f\"kernel: {kernel} - degree: {model.degree} - cost:{model.C}\"\n",
    "        elif kernel == 'rbf':\n",
    "            title = f\"kernel: {kernel}\"\n",
    "            if model.gamma != \"auto_deprecated\" :\n",
    "                title+= f\" - gamma: {model.gamma}\" \n",
    "            title += f\" - cost: {model.C}\"\n",
    "                \n",
    "        \n",
    "        for i in range(col): \n",
    "            if col>1:\n",
    "                ax = axs[i]\n",
    "            else:\n",
    "                ax = axs\n",
    "            ax.set_aspect('equal')\n",
    "            if i==0 and plot_boarders:\n",
    "                ax.set_title('Margins - ' + title,fontsize=plot_size*2)\n",
    "                boarders = model.decision_function(grid)\n",
    "                mask_pos = boarders >= 1\n",
    "                mask_neg = boarders <= -1\n",
    "                ax.scatter(grid[mask_pos,0], grid[mask_pos,1], c=color[0], alpha=0.01, s=10)\n",
    "                ax.scatter(grid[mask_neg,0], grid[mask_neg,1], c=color[1], alpha=0.01, s=10)\n",
    "                ax.scatter(X[mask_sv,0], X[mask_sv,1], c='g',label= str(np.sum(model.n_support_)) +' SV',s=40,marker='o')\n",
    "            if plot_classification and (i==1 or not plot_boarders):\n",
    "                ax.set_title('Classification - ' + title,fontsize=plot_size*2)\n",
    "                classification = model.predict(grid)\n",
    "                mask_pos = classification > 0\n",
    "                mask_neg = classification < 0\n",
    "                ax.scatter(grid[mask_pos,0], grid[mask_pos,1], c=color[0], alpha=0.01, s=10)\n",
    "                ax.scatter(grid[mask_neg,0], grid[mask_neg,1], c=color[1], alpha=0.01, s=10)\n",
    "                classification = model.predict(X)\n",
    "                mask_wrong = classification != y \n",
    "                ax.scatter(X[mask_wrong,0], X[mask_wrong,1], c='magenta',label=str(np.sum(mask_wrong)) + ' faults',s=40,marker='o')\n",
    "            m = y > 0\n",
    "            ax.scatter(X[m,0], X[m,1], c=color[0],label='class +1',s=10)\n",
    "            m = np.logical_not(m)\n",
    "            ax.scatter(X[m,0], X[m,1], c=color[1],label='class -1',s=10)\n",
    "            ax.legend(loc='lower left', fontsize=plot_size*1.5)   \n",
    "    \n",
    "    \n",
    "    else:\n",
    "        fig,axs = plt.subplots(1,1,figsize=(plot_size,plot_size))\n",
    "        axs.set_aspect('equal')\n",
    "        m = y > 0\n",
    "        axs.scatter(X[m,0], X[m,1], c=color[0],label='class +1',s=10)\n",
    "        m = np.logical_not(m)\n",
    "        axs.scatter(X[m,0], X[m,1], c=color[1],label='class -1',s=10)\n",
    "        axs.legend(loc='lower left', fontsize=plot_size*1.5)\n",
    "        plt.title(title, fontsize=plot_size*2)\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "loaddata"
    ]
   },
   "outputs": [],
   "source": [
    "#load data\n",
    "#leave as it is\n",
    "X,y = load_data(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": [
     "plotdata"
    ]
   },
   "outputs": [],
   "source": [
    "#plot your data ↓↓↓\n",
    "plot_data(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": [
     "iter_Degree"
    ]
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function iter_Degree fits SVM using defined range of degrees and plots every variation.\n",
    "@ degree_range: range of integer degrees\n",
    "@ returns list of dictionaries, lenght of list = length of degree_range\n",
    "\"\"\"\n",
    "def iter_Degree(degree_range, X, y):\n",
    "    list_of_models = []\n",
    "    #your code ↓↓↓\n",
    "    #code ends here\n",
    "    \n",
    "    return list_of_models\n",
    "\n",
    "#######################################################################################\n",
    "#using the written function\n",
    "\n",
    "degree_range = None #change this variable\n",
    "iter_Degree(degree_range,X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(210,90,80)\">Question 1 (5 points):</h3>\n",
    "\n",
    "What observations can you make from your plots? (several answers may be correct)\n",
    "\n",
    "\n",
    "_a) The SVM with polynomial degree=2 already seems to do quite well on the data set. </u><br>\n",
    "_b)  The higher the polynomial degree, the better the classifier. <br>\n",
    "_c)  A very high number of support vectors seems to be an indicator of a bad choice of the kernel. </u><br>\n",
    "\n",
    "\n",
    "_d) There is hardly any difference between the pictures that were produced by polynomial kernels of even degree.</u> <br>\n",
    "_e)  For kernels with an odd degree the number of misclassified samples decreases with an increasing degree. <br>\n",
    "\n",
    "To answer the question, assign \"True\" or \"False\" boolean values to variables in the next cell. A non-correctly answered question yields negative points and no answer (i.e. answer “None”) gives 0 points for a question. More details on grading can be found in the [FAQ sheet](https://docs.google.com/document/d/11ccAoEWh1APAoj79kGFiL64_7OL7RApPUHJ2-cvS2s0/edit?usp=sharing).<br>\n",
    "<b>Note:</b> Do not reuse these variable names. They are used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "question1"
    ]
   },
   "outputs": [],
   "source": [
    "example_True = True\n",
    "example_False = False\n",
    "\n",
    "a_ = None\n",
    "b_ = None\n",
    "c_ = None\n",
    "\n",
    "d_ = None\n",
    "e_ = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now apply an RBF-kernel and vary the $C$-parameter from close to $0$ to a very high value. After playing around a little bit, try to report on your observations in the subsequent question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": [
     "playground"
    ]
   },
   "outputs": [],
   "source": [
    "#Here you can play around with the code a little bit. Cell will not be used for grading.\n",
    "\n",
    "#SOLUTION\n",
    "for C in (0.1,1,10,100,1000):\n",
    "    model = svm.SVC(kernel=\"rbf\",C=C)\n",
    "    model.fit(X,y)\n",
    "    plot_data(X,y,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(210,90,80)\">Question 2 (5 points):</h3>\n",
    "\n",
    "What observations can you make from your plots? (several options may be correct):\n",
    "\n",
    "    \n",
    "_f)  The higher the cost the more support vectors we have. <br>\n",
    "_g)  The decision boarders don't change drastically with increasing $C$, only the number of support vectors does.</u> <br>\n",
    "\n",
    "\n",
    "To answer the question, assign \"True\" or \"False\" boolean values to variables in the next cell. A non-correctly answered question yields negative points and no answer (i.e. answer “None”) gives 0 points for a question. More details on grading can be found in the [FAQ sheet](https://docs.google.com/document/d/11ccAoEWh1APAoj79kGFiL64_7OL7RApPUHJ2-cvS2s0/edit?usp=sharing).<br>\n",
    "<b>Note:</b> Do not reuse these variable names. They are used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "question2"
    ]
   },
   "outputs": [],
   "source": [
    "example_True = True\n",
    "example_False = False\n",
    "\n",
    "f_ = None\n",
    "g_ = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(210,90,80)\">Code 2 (10 points):</h3>\n",
    "\n",
    "* Now use function `pol2cart()` which returns **100 two-dimensional** points which are **uniformly** distributed within the circle of radius $r=0.3$ to generate new data points. Label these points with $-1$ and add them to the main `X` feature matrix and `y` label vector. Your new variables will be `X_new` and `y_new`\n",
    "* Now use an **rbf kernel** and again play around with the parameter to explore the effects on the classification performance by again appropriately using the `plot_data` function. Write a function `iter_Gamma`, analogous to `iter_Degree`. Try out small costs $C \\sim 0.1$ and large costs $ C\\sim 1000$ and iterating over different values of $\\gamma := 1/(2\\sigma^2)$ (compare RBF definition in lecture slides), ranging from $0.1$ to $1$. Again report your observations in the subsequent question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": [
     "datareload"
    ]
   },
   "outputs": [],
   "source": [
    "#code that should help you\n",
    "#leave as it is\n",
    "X,y=load_data(1)\n",
    "def pol2cart(r, phi):\n",
    "    x = r * np.cos(phi)\n",
    "    y = r * np.sin(phi)\n",
    "    return(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "newdata"
    ]
   },
   "outputs": [],
   "source": [
    "#define new data ↓↓↓\n",
    "X_new, y_new = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "plotnewdata"
    ]
   },
   "outputs": [],
   "source": [
    "#plot new data ↓↓↓"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": [
     "iter_Gamma"
    ]
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function iter_Gamma fits SVM using defined gamma range and plots every variation.\n",
    "@ gamma_range: list of gammas\n",
    "@ returns: list of dictionaries, lenght of list = length of gamma_range\n",
    "\"\"\"\n",
    "def iter_Gamma(gamma_range,C,X_new,y_new):\n",
    "    list_of_models = []\n",
    "    #your code ↓↓↓\n",
    "    #code ends here\n",
    "    \n",
    "    return list_of_models\n",
    "\n",
    "#######################################################################################\n",
    "#using the written function\n",
    "\n",
    "cost_values = range(-1,4)\n",
    "gamma_values = [0.1,0.5,0.9]\n",
    "\n",
    "for cost in cost_values:\n",
    "    iter_Gamma(gamma_range,10**cost,X_new,y_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(210,90,80)\">Question 3 (5 points):</h3>\n",
    "\n",
    "What observations can you make from your plots? Tick the correct boxes:\n",
    "\n",
    "\n",
    "_h) For a fixed $\\gamma$, the larger the cost, the higher the number of support vectors .<br>\n",
    "_i )   For relatively large $C$ and relatively large $\\gamma$ (say $C \\geq 100$ and $\\gamma > 0.5$), enlarging $\\gamma$ further doens't improve your performance significantly. </u><br>\n",
    "_j ) For fixed $C$, increasing $\\gamma$ usually reduces the model complexity of the SVM. <br>\n",
    "\n",
    "To answer the question, assign \"True\" or \"False\" boolean values to variables in the next cell. A non-correctly answered question yields negative points and no answer (i.e. answer “None”) gives 0 points for a question. More details on grading can be found in the [FAQ sheet](https://docs.google.com/document/d/11ccAoEWh1APAoj79kGFiL64_7OL7RApPUHJ2-cvS2s0/edit?usp=sharing).<br>\n",
    "<b>Note:</b> Do not reuse these variable names. They are used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "question3"
    ]
   },
   "outputs": [],
   "source": [
    "example_True = True\n",
    "example_False = False\n",
    "\n",
    "h_ = None\n",
    "i_ = None\n",
    "j_ = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(210,90,80)\">Code 3 (10 points):</h3>\n",
    "\n",
    "* Finally, we want to investigate, how the RBF kernel classifier reacts to outliers. To this end add a single point $(1.8,1.2)$ to your new data set (with the additional circle around $0$) and label it with $y=+1$. Again plot the data set.?\n",
    "* Again use an RBF kernel and play around with the parameter to explore the effects on the classification performance by again appropriately using the `plot_data` function. Try out small costs $C \\sim 0.1$ and large costs $ C\\sim 1000$ and also iterate over different values of $\\gamma$, ranging from $0.1$ to $1$. (Reuse iterative functions defined above). Report your observations in the subsequent question. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "onemorepoint"
    ]
   },
   "outputs": [],
   "source": [
    "# update X_new and y_new by adding point as described in the task↓↓↓\n",
    "X_new = None\n",
    "y_new = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": [
     "playground"
    ]
   },
   "outputs": [],
   "source": [
    "#This cell is just to discover the behaviour of SVM given extra point, not graded.\n",
    "\n",
    "#code here ↓↓↓"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3 style=\"color:rgb(210,90,80)\">Question 4 (5 points):</h3>\n",
    "\n",
    "What observations can you make from your plots? Tick the correct boxes:\n",
    "\n",
    "    \n",
    "_k)  For relatively low costs (e.g. $C\\le 1$) and an appropriately chosen $\\gamma$ (e.g. $0.9$) the classifier correctly classifies all points except the outlier. </u><br>\n",
    "_l)  For relatively high costs (e.g. $C\\geq 100$) the classifier always, i.e. independent of $\\gamma$, shows a region of the positive class near the outlier.<br>\n",
    "_m)  Classifiers with high costs ($C\\geq 100$ say) and high $\\gamma$ ($ \\gamma \\geq 0.5$) are susceptible to overfitting.</u><br>\n",
    "\n",
    "To answer the question, assign \"True\" or \"False\" boolean values to variables in the next cell. A non-correctly answered question yields negative points and no answer (i.e. answer “None”) gives 0 points for a question. More details on grading can be found in the [FAQ sheet](https://docs.google.com/document/d/11ccAoEWh1APAoj79kGFiL64_7OL7RApPUHJ2-cvS2s0/edit?usp=sharing).<br>\n",
    "<b>Note:</b> Do not reuse these variable names. They are used for testing.\n",
    "\n",
    "You are of course encouraged to further apply the given plot routine to further datasets and different hyperparameters to get further intuition!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "question4"
    ]
   },
   "outputs": [],
   "source": [
    "example_True = True\n",
    "example_False = False\n",
    "\n",
    "k_ = None\n",
    "l_ = None\n",
    "m_ = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "playground"
    ]
   },
   "outputs": [],
   "source": [
    "#here you can experiment, cell is not graded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\\color:rgb(0,120,170)\\>Technical cells</h2>\n",
    "    The cells below are needed for efficient unittesting.\n",
    "    Do not delete or change in order to receive proper evaluation.<br>\n",
    "    Executability check might help you with datatypes, but does not guarantee your answers are 100% correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "techcells"
    ]
   },
   "outputs": [],
   "source": [
    "import matplotlib.figure\n",
    "import matplotlib.collections\n",
    "import matplotlib.axes\n",
    "def X_y_tolist(X,y):\n",
    "    return X.tolist(), y.tolist()\n",
    "\n",
    "def testoptions(options):\n",
    "    for elem in options:\n",
    "        if elem!=True and elem!=False and elem!=None:\n",
    "            raise ValueError(f\"Check answers for questions again\")\n",
    "    print(\"Test questions answers are ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    X_y_tolist(X,y)\n",
    "except:\n",
    "    raise ValueError(\"Check your X and y variables\")\n",
    "try:\n",
    "    X_y_tolist(X_new,y_new)\n",
    "except:\n",
    "    raise ValueError(\"Check your X_new and y_new variables\")\n",
    "testoptions(np.array([a_,b_,c_,d_,e_,f_,g_,h_,i_,j_,k_,l_,m_]))"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
